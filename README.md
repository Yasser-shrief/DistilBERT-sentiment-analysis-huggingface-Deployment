# DistilBERT-sentiment-classification-huggingface-Deployment
Deploy a Flask DistilBERT App on AWS EC2 Instance/local machine

## What is DistilBERT?  

BERT is designed to pretrain deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial taskspecific architecture modifications.<br>

DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bertâ€™s performances as measured on the GLUE language understanding benchmark.

![Alt text](https://github.com/Yasser-shrief/Text-Summarization_-TensorFlow-/blob/main/Models.JPG)
